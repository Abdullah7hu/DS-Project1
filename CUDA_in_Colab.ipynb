{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install CUDA C++ plugin for Colab:\n",
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntOgKF_-y0SV",
        "outputId": "4fd229dc-fe3d-418f-eea5-07ca001fc919"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nvcc4jupyter in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "The nvcc4jupyter extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc4jupyter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect selected GPU and its NVIDA architecture:\n",
        "import subprocess\n",
        "gpu_info = subprocess.getoutput(\"nvidia-smi --query-gpu=name,compute_cap --format=csv,noheader,nounits\")\n",
        "if \"not found\" in gpu_info.lower(): raise RuntimeError(\"Error: No GPU found. Please select a GPU runtime environment.\")\n",
        "gpu_name, compute_cap = map(str.strip, gpu_info.split(','))\n",
        "gpu_arch = f\"sm_{compute_cap.replace('.', '')}\"\n",
        "\n",
        "print(f\"{'GPU Name':<15}: {gpu_name}\")\n",
        "print(f\"{'Architecture':<15}: {gpu_arch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVqh0baiO15d",
        "outputId": "75a882c4-966d-43fd-a814-2c187a9bdd4b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Name       : Tesla T4\n",
            "Architecture   : sm_75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -c \"--gpu-architecture sm_80\"\n",
        "#include <stdio.h>\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "\n",
        "__global__ void bitonic_stage(int *data, int n, int i, int j){\n",
        "    int k = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (k >= n) return;\n",
        "\n",
        "    // size = 2^i, stride = 2^(j-1)\n",
        "    int size   = 1 << i;\n",
        "    int stride = 1 << (j - 1);\n",
        "\n",
        "    // participation: only threads in the lower 'stride' part of each size-block act\n",
        "    if ((k & (size - 1)) >= stride) return;\n",
        "\n",
        "    // correct partner pairing (XOR with stride)\n",
        "    int partner = k ^ stride;\n",
        "    if (partner >= n) return;\n",
        "\n",
        "    int region    = k >> i;             // = k / size\n",
        "    int ascending = ((region & 1) == 0);\n",
        "\n",
        "    int a = data[k];\n",
        "    int b = data[partner];\n",
        "\n",
        "    if (ascending) {\n",
        "        if (a > b) {\n",
        "            data[k]       = b;\n",
        "            data[partner] = a;\n",
        "        }\n",
        "    } else {\n",
        "        if (a < b) {\n",
        "            data[k]       = b;\n",
        "            data[partner] = a;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "void bitonic_sort_gpu(int *h_arr, int n){\n",
        "    int *d_arr;\n",
        "    cudaMalloc(&d_arr, n * sizeof(int));\n",
        "    cudaMemcpy(d_arr, h_arr, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threads = 256;\n",
        "    int blocks  = (n + threads - 1) / threads;\n",
        "\n",
        "    // steps = log2(n) for n power of 2\n",
        "    int steps = 0;\n",
        "    for (int tmp = n; tmp > 1; tmp >>= 1) {\n",
        "        steps++;\n",
        "    }\n",
        "\n",
        "    for (int i = 1; i <= steps; i++) {\n",
        "        for (int j = i; j >= 1; j--) {\n",
        "            bitonic_stage<<<blocks, threads>>>(d_arr, n, i, j);\n",
        "            cudaDeviceSynchronize();\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(h_arr, d_arr, n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaFree(d_arr);\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "\n",
        "    // Array size MUST be a power of 2 (here 8)\n",
        "    int arr[] = {3, 7, 8, 9, 5, 4, 3, 2};\n",
        "    int n = sizeof(arr) / sizeof(arr[0]);\n",
        "\n",
        "    printf(\"Unsorted: \");\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        printf(\"%d \", arr[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    bitonic_sort_gpu(arr, n);\n",
        "\n",
        "    printf(\"Sorted:   \");\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        printf(\"%d \", arr[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HW8N8RXzCTc",
        "outputId": "d747e930-5a8b-4d04-99f0-39045e33509a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsorted: 3 7 8 9 5 4 3 2 \n",
            "Sorted:   3 7 8 9 5 4 3 2 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HW8N2RXzCTc",
        "outputId": "06b815c1-d819-4030-a53d-5f0ae894c2e3"
      },
      "source": [
        "%%cuda -c \"--gpu-architecture sm_80\"\n",
        "#include <stdio.h>\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "\n",
        "__global__ void bitonic_stage(int *data, int n, int i, int j){\n",
        "\n",
        "    int k = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (k >= n) return;\n",
        "\n",
        "    // 'i' corresponds to the log2 of the current bitonic sequence length (2^i)\n",
        "    // 'j' corresponds to the log2 of the comparison distance (2^(j-1))\n",
        "\n",
        "    int current_len = 1 << i;          // The length of the current bitonic sequence block\n",
        "    int comp_dist   = 1 << (j - 1);    // The comparison distance\n",
        "\n",
        "    // Determine the sorting direction for this 'current_len'-sized block\n",
        "    // If k is in an even 'current_len'-sized block, sort ascending. Else descending.\n",
        "    int ascending = ((k / current_len) % 2 == 0); // Equivalent to ( (k >> i) & 1 ) == 0\n",
        "\n",
        "    // This thread only participates if 'k' is in the first half of a 'comp_dist * 2' segment.\n",
        "    if ((k % (comp_dist * 2)) >= comp_dist) return;\n",
        "\n",
        "    int partner = k + comp_dist;\n",
        "    if (partner >= n) return;\n",
        "\n",
        "    int a = data[k];\n",
        "    int b = data[partner];\n",
        "\n",
        "    if (ascending) {\n",
        "        if (a > b) {\n",
        "            data[k]       = b;\n",
        "            data[partner] = a;\n",
        "        }\n",
        "    } else {\n",
        "        if (a < b) {\n",
        "            data[k]       = b;\n",
        "            data[partner] = a;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "void bitonic_sort_gpu(int *h_arr, int n)\n",
        "{\n",
        "    int *d_arr;\n",
        "    cudaMalloc(&d_arr, n * sizeof(int));\n",
        "    cudaMemcpy(d_arr, h_arr, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threads = 256;\n",
        "    int blocks  = (n + threads - 1) / threads;\n",
        "\n",
        "    // steps = log2(n) for n power of 2\n",
        "    int steps = 0;\n",
        "    for (int tmp = n; tmp > 1; tmp >>= 1) {\n",
        "        steps++;\n",
        "    }\n",
        "\n",
        "    for (int i = 1; i <= steps; i++) {\n",
        "        for (int j = i; j >= 1; j--) {\n",
        "            bitonic_stage<<<blocks, threads>>>(d_arr, n, i, j);\n",
        "            cudaDeviceSynchronize();\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(h_arr, d_arr, n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaFree(d_arr);\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "\n",
        "    // Array size MUST be a power of 2 (here 8)\n",
        "    int arr[] = {3, 7, 8, 9, 5, 4, 3, 2};\n",
        "    int n = sizeof(arr) / sizeof(arr[0]);\n",
        "\n",
        "    printf(\"Unsorted: \");\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        printf(\"%d \", arr[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    bitonic_sort_gpu(arr, n);\n",
        "\n",
        "    printf(\"Sorted:   \");\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        printf(\"%d \", arr[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsorted: 3 7 8 9 5 4 3 2 \n",
            "Sorted:   3 7 8 9 5 4 3 2 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}